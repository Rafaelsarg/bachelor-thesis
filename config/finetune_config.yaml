# General metadata
project_name: "Thesis - Safety Classification"
run_name: "${model_id}-${task}-${model_type}-lr${sft_config.lr}-bs${sft_config.batch_size}-${now:%Y%m%d_%H%M%S}"
group: "${task}"                 # safety, cluster
tags: ["${model_id}", "${task}", "qlora", "lora"]
task: "cluster"
hf_token: "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"

# Model selector (choose in CLI)
model_id: "llama3"  # or "llama3", "phi"

# Model names per LLM
model_map:
  mistral: "mistralai/Mistral-7B-Instruct-v0.2"
  llama3: "meta-llama/Meta-Llama-3-8B-Instruct"
  phi: "microsoft/Phi-3-mini-4k-instruct"

# Model Type
model_type: "classification"  # or "casual"

dataset_config:
  dataset_map:
    safety: "data/processed/safety-dataset-90-5-5.hf"
    cluster: "data/processed/cluster-dataset-70-25-5.hf"
  dataset_path: "${dataset_config.dataset_map[${task}]}"
  num_labels_map:
    safety: 2
    cluster: 12
  num_labels: ${dataset_config.num_labels_map[${task}]}
  label_names_map:
    safety: ["Unsafe", "Safe"]
    cluster:
      - "DIET_PLAN_ISSUES"
      - "SOCIAL"
      - "SITUATIONAL"
      - "MOTIVATION"
      - "EMOTIONS"
      - "CRAVING_HABIT"
      - "MENTAL_HEALTH"
      - "ENERGY_EFFORT_CONVENIENCE"
      - "PORTION_CONTROL"
      - "KNOWLEDGE"
      - "HEALTH_CONDITION"
      - "NOT_APPLICABLE"
  label_map:
    safety:
      label2id:
        Unsafe: 0
        Safe: 1
    cluster:
      label2id:
        DIET_PLAN_ISSUES: 0
        SOCIAL: 1
        SITUATIONAL: 2
        MOTIVATION: 3
        EMOTIONS: 4
        CRAVING_HABIT: 5
        MENTAL_HEALTH: 6
        ENERGY_EFFORT_CONVENIENCE: 7
        PORTION_CONTROL: 8
        KNOWLEDGE: 9
        HEALTH_CONDITION: 10
        NOT_APPLICABLE: 11
  label2id: ${dataset_config.label_map[${task}].label2id}
  label_names: ${dataset_config.label_names_map[${task}]}

output_dir: "results/finetuning/${model_id}/${task}/${run_name}"

# SFT config
sft_config:
  sft_cfg:
  batch_size: 16               
  epochs: 10              
  lr: 2e-5                  
  evaluation_strategy: "epoch"
  gradient_accumulation_steps: 2  
  max_grad_norm: 0.3        
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.05
  optim: "paged_adamw_32bit"  
  logging_steps: 10
  max_seq_length: 512
  weight_decay: 0.005 
  loss_function: "custom"   

# LoRA config (you can override these per model in CLI or a model-specific override file)
lora_config:
  r: 8
  alpha: 16
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  dropout: 0.05
